# Azure for the Data Engineer

## Evolving world of data

Some major important developments are different types of data (structured, unstructured, aggregated), systems must be accurate, secure, have high availability, GDPR compliant. On premises has problems that cloud doesn't have (of course)

A data engineer extracts raw data from a data pool and migrates it to a staging data repository. There is some transforming done. It is then loaded into the data warehouse. ETL baby!
Alternatively, you can do ELT, it is immediately loaded into Cosmos or Data Lake, transforming is done after the load is complete.
ELT is more common in the cloud, since you want to get all the data into the cloud first.

Also, not Azure specific: OLTP is processing focused on transactions. Generally, you will have many CRUD operations throughout the day that are not neccecarily related. Mainly you have many small transactions by many different users. OLAP is analytics processing, is more about answering analytical questions, that may concern larger amounts of data.

Azure is great for data engineering because you use a web interface, can script deployments, and get globally distributed available databases easily.

## Surveying services on Azure

Data types are Structured, Nonstructured (key-value, documents, graph, column store)

Azure Storage has Blob (text/binary), Files, Queue, Table (NoSQL for no-schema structured data)

- Blob storage: for storing but not querying data. Is the cheapest way to store data in Azure. To ingest data, use DataFactory, Storage Explorer, AzCopy, Powerhell, or Visual Studio, or just the Rest API. Maximum file size is 1TB. If you really want to query the data, consider Data Lake Storage. Security is done with SAS tokens, or RBAC.

- Data Lake Storage Gen2 makes use of Blob storage, but also a storage layer for DataBricks, Hadoop, HDInsight. Some features:
    - Unlimited scalability
    - Hadoop compatible
    - file system designed for big-data analytics
    - Zone redundant, geo-redundant
    - You can query with Blob storage API or ADLS (azure data lake system) API

- CosmosDB is a NoSQL store at planet scale with low latency and 99.999 SLA. It's multimaster with different consistency levels. You can query with Javascript or other APIs

- Azure SQL Database is basically a more scalable version of an on-premise SQL server. Mainly for OLTP. It uses T-SQL.

- Azure Synapse Analytics is a combination of Enterprise Data Warehousing and Big Data Analytics. It can answer complex business question with limitless scale. It can run queries across petabytes of data. You can scale compute nodes independently of storage. It uses distributed tables (round robin, hash or replicated) to coordinate data between compute nodes. You can also pause computation nodes and you don't pay for paused nodes. (e.g. only run computations at night) You can use T-SQL to query contents of Synapse.

- Azure Stream Analytics is a gateway for data streams such as IoT Hub and Event Hubs. It monitors the streams in real time for anomalies, and also stores them in a DataLake or CosmosDB or PowerBI (or others). You should mainly use it if you want to respond to events in real-time or analyse data (in batch) as it comes in. (example: if you want to queue data from IoT Hub and just make it come in every hour and analyse/aggregate it.)
    - IoT hub has features to work with IoT devices specifically
    - Event hubs is for high data throughput (billiopns of requests per day)
I will definitely read about Stream analytics more, it's not 100% clear to me yet.

- Azure HDInsight is for ingesting/processing/analyzing big data. It includes Hadoop/Spark/Kafka/HBase/Storm. I'm not 100% sure how this is different from Synapse.

- DataBricks is serverless for Spark-based applications, including an interactive workspace and a REST API for programming clusters. You can use R/Python/Scala/SQL.

- DataFactory is for orchestrating movement between various data stores. You can tranform data with HDInsight, Hadoop, Spark, AzureML.

- DataCatalog is for crowdsourcing metadata about data sources. Kind of like a wiki for storing documentation about data sources within an organiation.

## Tasks of the Data Engineer

A Data engineer sets up technologies to secure the flow of data. They ingest/egress/transform data from multiple sources. It's different from a DBAdmin. They must also ingest/tansform/validate/clean data (data wrangling)
An AI Engineer uses cognitive sertvies, and uses prebuilt capabilities of Cognitive Services API. They rely on data engineers to store information generated by AI.
Data Scientists perform descriptive/predictive analytics. They rely on data engineers when the data engineers partly/largely do a lot of data wrangling for them.

Some data engineering processes are ETL (not gonna explan again). ELT is an evolution of ETL: you can store data in its original format, and define the data structure during the transformation step. This is mainly more practical now that Azure can handle unstructured data at an unlimited scale. You don't lose data - if one application needs a field that another application doesn't, you just change the transformation slightly. The original data is still there (unlike with ETL, where you need to change the T, you need to do the entire L all over again)

Holistic Data Engineering goes as follows:
    - Source: Identify the source systems
    - Ingest: Identify how to load the data
    - Prepare: Identify how to transform the data
After that is sorted
    - Analyse: identify how to analyse the data
    - Consume: Identify how to consume/present the data

This can often lead to an ELTL process, where e.g. the first L is to Data Lake, the second L is to Data Warehouse.
